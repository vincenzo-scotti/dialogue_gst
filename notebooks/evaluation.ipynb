{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "import bz2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mellotron_api import load_tts, load_vocoder, load_arpabet_dict\n",
    "from mellotron_api import get_gst_embeddings, get_gst_scores\n",
    "from mellotron_api.api import _synthesise_speech_mellotron"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gsttransformer.speech_api import ChatSpeechGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = '../resources/data/raw'\n",
    "DATA_PATH = '../resources/data/cache'\n",
    "OUTPUT_PATH = '../resources/data/eval'\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'validation', 'test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUT_DF_COLUMNS = [\n",
    "    'Model', 'Params [M]', 'Approach', 'Split', 'Audio file path', 'MSE', 'KL-Divergence', 'Frobenius norm (embeddings)', 'Frobenius norm (combination weights)'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_PATH = ''\n",
    "THERAPY_MODEL_PATH = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with bz2.BZ2File(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = {\n",
    "    split: load_data(os.path.join(DATA_PATH, f'gstt_corpus_{split}.pbz2'))\n",
    "    for split in SPLITS[1:]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mellotron, mellotron_stft, mellotron_hparams = load_tts('resources/tts/mellotron/mellotron_libritts.pt')\n",
    "tacotron2, tacotron2_stft, tacotron2_hparams = load_tts('resources/tts/tacotron_2/tacotron2_statedict.pt', model='tacotron2')\n",
    "waveglow, denoiser = load_vocoder('resources/vocoder/waveglow/waveglow_256channels_universal_v4.pt')\n",
    "arpabet_dict = load_arpabet_dict('mellotron/data/cmu_dictionary')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "language_models = {\n",
    "    'DialoGPT (117M)': 'microsoft/DialoGPT-small',\n",
    "    'DialoGPT (345M)': 'microsoft/DialoGPT-medium',\n",
    "    'DialoGPT (762M)': 'microsoft/DialoGPT-large',\n",
    "    'Therapy-DLDLM': ''\n",
    "}\n",
    "\n",
    "model_size_mapping = {\n",
    "    'DialoGPT-117M': 117,\n",
    "    'DialoGPT-345M': 345,\n",
    "    'DialoGPT-762M': 762,\n",
    "    'Therapy-DLDLM': 762\n",
    "}\n",
    "\n",
    "dgpt_mapping = {\n",
    "    'lm_small': 'DialoGPT-117M',\n",
    "    'lm_medium': 'DialoGPT-345M',\n",
    "    'lm_large': 'DialoGPT-762M'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "approaches_mapping = {\n",
    "    'resp': 'Response',\n",
    "    'resp_from_ctx': 'Response (from context)',\n",
    "    'ctx_resp': 'Context and response'\n",
    "}\n",
    "\n",
    "dgst_models_dict = {\n",
    "    **{\n",
    "        (dgpt_mapping[dgpt_mapping[model]]): (\n",
    "            os.path.join(MODEL_PATH, 'model', f'best_checkpoint_{model}_{approach}', 'gstt.pt'),\n",
    "            language_models[dgpt_mapping[model]],\n",
    "            'gpt2',\n",
    "            {'encoding_mode': approach, 'max_context_len': 256}\n",
    "        )\n",
    "        for model in dgpt_mapping for approach in approaches_mapping\n",
    "    },\n",
    "    **{\n",
    "        ('Therapy-DLDLM', approaches_mapping[approach]): (\n",
    "            os.path.join(THERAPY_MODEL_PATH, 'model', f'best_checkpoint_lm_large_{approach}', 'gstt.pt'),\n",
    "            language_models('Therapy-DLDLM'),\n",
    "            language_models('Therapy-DLDLM'),\n",
    "            {\n",
    "                'encoding_mode': approach,\n",
    "                'prefix_token': '<|prior|>',\n",
    "                'suffix_token': '<|posterior|>',\n",
    "                'max_context_len': 256\n",
    "            }\n",
    "        )\n",
    "        for approach in approaches_mapping\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "out_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    tgt_data = {\n",
    "        sample['audio_file_path']: (\n",
    "            get_gst_embeddings(\n",
    "                os.path.join(DATA_PATH, sample['audio_file_path']), mellotron, mellotron_stft, mellotron_hparams\n",
    "            ),\n",
    "            get_gst_scores(\n",
    "                os.path.join(DATA_PATH, sample['audio_file_path']), mellotron, mellotron_stft, mellotron_hparams\n",
    "            ),\n",
    "            _synthesise_speech_mellotron(\n",
    "                sample['utterance'], mellotron, mellotron_stft, mellotron_hparams, arpabet_dict,\n",
    "                reference_audio_path=os.path.join(DATA_PATH, sample['audio_file_path'])\n",
    "            )[1]\n",
    "        )\n",
    "        for split, samples in data.items() for sample in samples\n",
    "    }\n",
    "    for (lm_id, dgst_approach), (dgst_path, lm_path, tokeniser_path, kwargs) in dgst_models_dict.items():\n",
    "        dgst = ChatSpeechGenerator(\n",
    "            dgst_path, lm_path, tokeniser_path, **kwargs\n",
    "        )\n",
    "        for split, samples in data.items():\n",
    "            for sample in samples:\n",
    "                gst_embeddings, gst_scores = dgst._predict_gst(sample['utterance'], dialogue=sample['context'])\n",
    "                mel_spec_embeds = _synthesise_speech_mellotron(\n",
    "                    sample['utterance'], mellotron, mellotron_stft, mellotron_hparams, arpabet_dict,\n",
    "                    reference_audio_path=os.path.join(DATA_PATH, sample['audio_file_path']),\n",
    "                    gst_style_embedding=gst_embeddings\n",
    "                )[1]\n",
    "                mel_spec_weights = _synthesise_speech_mellotron(\n",
    "                    sample['utterance'], mellotron, mellotron_stft, mellotron_hparams, arpabet_dict,\n",
    "                    reference_audio_path=os.path.join(DATA_PATH, sample['audio_file_path']),\n",
    "                    gst_head_style_scores=gst_scores\n",
    "                )[1]\n",
    "\n",
    "                tgt_mel_spec, tgt_gst_embeddings, tgt_gst_scores = tgt_data[sample['audio_file_path']]\n",
    "\n",
    "                mse = F.mse_loss(\n",
    "                    torch.tensor(gst_embeddings),torch.tensor(tgt_gst_embeddings), reduction='none'\n",
    "                ).mean(-1)\n",
    "                kl = F.kl_div(\n",
    "                    torch.tensor(gst_scores).log(), torch.tensor(tgt_gst_scores).log(), reduction='none', log_target=True\n",
    "                ).sum(-1).mean(1)\n",
    "                frob_embeds = (((tgt_mel_spec - mel_spec_embeds) ** 2)).sum() ** 0.5\n",
    "                frob_weights = (((tgt_mel_spec - mel_spec_weights) ** 2)).sum() ** 0.5\n",
    "\n",
    "                out_data.append([\n",
    "                    lm_id, model_size_mapping[lm_id], dgst_approach, split, sample['audio_file_path'],\n",
    "                    mse.item(), kl.item(), frob_embeds.item(), frob_weights.item()\n",
    "                ])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(out_data, columns=OUT_DF_COLUMNS)\n",
    "out_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_df.to_csv(os.path.join(OUTPUT_PATH, 'results.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for split in out_df['Split'].unique():\n",
    "    for metric in ['MSE', 'KL-Divergence', 'Frobenius norm (embeddings)', 'Frobenius norm (combination weights)']:\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=3,\n",
    "            figsize=(12, 5),\n",
    "            sharex=True,\n",
    "            sharey=True\n",
    "        )\n",
    "        for i, approach in enumerate(['Response', 'Response (from context)', 'Context and response']):\n",
    "            order = ['DialoGPT-117M', 'DialoGPT-345M', 'DialoGPT-762M', 'Therapy-DLDLM']\n",
    "            sns.countplot(\n",
    "                data=out_df[(out_df['Split'] == split) & (out_df['Approach'] == approach)],\n",
    "                x=metric, y='Model', order=order, ax=axes[i], linewidth=1., edgecolor='0', orient='h', errorbar='sd'\n",
    "            )\n",
    "            axes[i].set_title(f'Approach: {approach}')\n",
    "            axes[i].set_xscale('log')\n",
    "            # axes[i].set_xlim([0, 1.])\n",
    "            axes[i].set_xlabel(metric)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig(\n",
    "            os.path.join(\n",
    "                OUTPUT_PATH,\n",
    "                f'dgst_results_{split.lower()}_{metric.lower(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")}.pdf'\n",
    "            ),\n",
    "            bbox_inches='tight'\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}