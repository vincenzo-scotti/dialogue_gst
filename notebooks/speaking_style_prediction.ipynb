{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Speech synthesis with text predicted GST\n",
    "In this notebook we show how to use the pretrained models. We use the best performing model\n",
    "\n",
    "This notebook has been partially taken from a notebook in the original repository on Tacotron 2 (with DDC) synthesis\n",
    "with GST, Speaker embeddings and vocoder ([link](https://colab.research.google.com/drive/1t0TFC3vqU1nFow5p5FTPjtkT6rFJOSsB?usp=sharing))\n",
    "and a notebook for speaker controlled speech synthesis\n",
    "([link](https://github.com/vincenzo-scotti/ITAcotron_2/blob/ITAcotron2/notebooks/ITAcotron-2_synthesis.ipynb)).\n",
    "\n",
    "Additional resources\n",
    "[link 1](https://colab.research.google.com/drive/1Gtt9EV1fFzuKbOdqUrLuAMuxBaot5v4F?usp=sharing),\n",
    "[link 2](https://colab.research.google.com/drive/1-xI9HiG5B-APnwe7KQwtOBPp1gggg-jZ?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.tts.utils.text.symbols import symbols, phonemes, make_symbols\n",
    "from TTS.utils.io import load_config  # Config class loader\n",
    "from TTS.tts.utils.generic_utils import setup_model  # TTS model setup\n",
    "from TTS.tts.utils.io import load_checkpoint  # Model checkpoint loader\n",
    "from TTS.vocoder.utils.generic_utils import setup_generator  # Vocoder model setup\n",
    "from TTS.tts.utils.synthesis import synthesis  # Main wrapper for speech synthesis\n",
    "from pathlib import Path\n",
    "from encoder import inference as speaker_encoder_model\n",
    "from encoder.params_model import model_embedding_size as speaker_embedding_size\n",
    "\n",
    "# from model import ...\n",
    "# from data import ...\n",
    "\n",
    "from typing import Optional, Union, ByteString, List, Dict, Callable\n",
    "from TTS.utils.io import AttrDict\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.vocoder.models.fullband_melgan_generator import FullbandMelganGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm_model_checkpoint_path = tokeniser_checkpoint_path = 'microsoft/DialoGPT-large'\n",
    "\n",
    "tts_model_configs_path = './resources/models/tts/config.json'\n",
    "tts_model_checkpoint_path = './resources/models/tts/best_model.pth.tar'\n",
    "tts_model_speaker_file = './resources/models/tts/speakers.json'\n",
    "tts_model_scale_stats_path = None\n",
    "\n",
    "speaker_encoder_model_checkpoint_path = './resources/models/speaker_encoder/pretrained.pt'\n",
    "\n",
    "vocoder_model_config_path = './resources/models/vocoder/config.json'\n",
    "vocoder_model_checkpoint_path = './resources/models/vocoder/best_model.pth.tar'\n",
    "vocoder_model_scale_stats_path = './resources/models/vocoder/scale_stats.npy'\n",
    "\n",
    "gstt_model_checkpoint_path = './resources/models/gstt/'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set random seed for reproducibility."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_seed = 2307\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate response given context."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "        context: List[str],\n",
    "        dialogue_lm: transformers.GPT2LMHeadModel,\n",
    "        tokeniser: transformers.GPT2Tokenizer,\n",
    "        generate_kwargs: Optional[Dict] = None\n",
    ") -> str:\n",
    "    ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Speaker embedding computation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_speaker_embedding(\n",
    "        reference_audio_path: Union[str, List[str]],\n",
    "        audio_loading_fn: Callable,\n",
    "        speaker_embedding_fn: Callable\n",
    ") -> List[float]:\n",
    "    if isinstance(reference_audio_path, str):\n",
    "        return extract_speaker_embedding([reference_audio_path], audio_loading_fn, speaker_embedding_fn)\n",
    "\n",
    "    speaker_embedding: List[float] = np.vstack([\n",
    "        speaker_embedding_fn(audio_loading_fn(audio_file_path)) for audio_file_path in reference_audio_path\n",
    "    ]).mean(axis=0).tolist()\n",
    "\n",
    "    return speaker_embedding\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GST extraction from reference audio."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_gst(\n",
    "        reference_audio_path:str,\n",
    "        gst_embedding_model: torch.nn.Module\n",
    ") -> Dict[str, float]:\n",
    "    ...\n",
    "    # Run GST extraction\n",
    "    gst = ...\n",
    "\n",
    "    gst = {str(i): value for i, value in enumerate(gst)}\n",
    "\n",
    "    # return gst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GST prediction from text:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_gst(\n",
    "        context: Optional[List[str]] = None,\n",
    "        response: Optional[str] = None,\n",
    "        contextual_embeddings: Optional[torch.tensor] = None,\n",
    ") -> Dict[str, float]:\n",
    "    # Input consistency check\n",
    "\n",
    "    # Extract input contextual embeddings if not provided as input\n",
    "    if contextual_embeddings is None:\n",
    "        contextual_embeddings = ...\n",
    "\n",
    "    # Run GST prediction\n",
    "    gst = ...\n",
    "\n",
    "    # Convert to desired data format\n",
    "    gst = {str(i): value for i, value in enumerate(gst.cpu().squeeze().tolist())}\n",
    "\n",
    "    return gst\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text-to-Speech function, given the text and, possibly, the reference speaker embeddings and GST and it generates the audio waveform."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tts(\n",
    "        text: str,\n",
    "        tts_model: Tacotron2,\n",
    "        tts_configs: AttrDict,\n",
    "        tts_ap: Optional[AudioProcessor] = None,\n",
    "        audio_loading_fn: Optional[Callable] = None,\n",
    "        speaker_embedding_fn: Optional[Callable] = None,\n",
    "        vocoder_model: Optional[FullbandMelganGenerator] = None,\n",
    "        vocoder_configs: Optional[AttrDict] = None,\n",
    "        vocoder_ap: Optional[AudioProcessor] = None,\n",
    "        speaker_reference_clip_path: Optional[Union[List[str], str]] = None,\n",
    "        speaker_embeddings: Optional[List[float]] = None,\n",
    "        gst_reference_clip_path: Optional[str] = None,\n",
    "        gst_style: Optional[Dict[str, float]] = None\n",
    ") -> np.ndarray:\n",
    "    # Input consistency check\n",
    "    assert speaker_reference_clip_path is None or speaker_embeddings is None\n",
    "    assert gst_reference_clip_path is None or gst_style is None\n",
    "    # Helper function for the Vocoder issue\n",
    "    def interpolate_vocoder_input(scale_factor, spec):\n",
    "        spec = torch.tensor(spec).unsqueeze(0).unsqueeze(0)\n",
    "        spec = torch.nn.functional.interpolate(spec, scale_factor=scale_factor, mode='bilinear').squeeze(0)\n",
    "        return spec\n",
    "\n",
    "    # Prepare inputs for actual synthesis\n",
    "    use_cuda = device.type == 'cuda'\n",
    "    use_gl = vocoder_model is None\n",
    "    if speaker_reference_clip_path is not None:\n",
    "        speaker_embeddings = extract_speaker_embedding(\n",
    "            speaker_reference_clip_path, audio_loading_fn, speaker_embedding_fn\n",
    "        )\n",
    "    if gst_reference_clip_path is not None:\n",
    "        gst_style = gst_reference_clip_path\n",
    "\n",
    "    # Audio synthesis step\n",
    "    waveform, _, _, mel_postnet_spec, _, _ = synthesis(\n",
    "        tts_model,\n",
    "        text,\n",
    "        tts_configs,\n",
    "        use_cuda,\n",
    "        tts_ap,\n",
    "        style_wav=gst_style,\n",
    "        use_griffin_lim=use_gl,\n",
    "        speaker_embedding=speaker_embeddings\n",
    "    )\n",
    "\n",
    "    # Postprocessing\n",
    "    # if tts_configs.model == \"Tacotron\" and not use_gl:\n",
    "    #     mel_postnet_spec = tts_model.ap.out_linear_to_mel(mel_postnet_spec.T).T\n",
    "    if not use_gl:\n",
    "        mel_postnet_spec = tts_ap._denormalize(mel_postnet_spec.T).T\n",
    "        vocoder_input = vocoder_ap._normalize(mel_postnet_spec.T)\n",
    "\n",
    "        output_scale_factor = vocoder_configs.audio.sample_rate / tts_configs.audio.sample_rate\n",
    "        if output_scale_factor != 1.:\n",
    "            scale_factor = [1., output_scale_factor]\n",
    "            vocoder_input = interpolate_vocoder_input(scale_factor, vocoder_input)\n",
    "        else:\n",
    "            vocoder_input = torch.FloatTensor(vocoder_input).unsqueeze(0)\n",
    "        waveform = vocoder_model.inference(vocoder_input)\n",
    "    if use_cuda and not use_gl:\n",
    "        waveform = waveform.cpu()\n",
    "    if not use_gl:\n",
    "        waveform = waveform.numpy()\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    return waveform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dialogue printing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_dialogue(context: List[str], response: str):\n",
    "    text = 'Context:\\n' + '\\t'.join(f'- {line}\\n' for line in context) + '\\n' + f'Response:\\n\\t- {response}'\n",
    "    print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate playable audio from waveform inside a Jupyter notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def play_audio(waveform: Union[np.ndarray, List[float], str, ByteString], sr: Optional[int] = None):\n",
    "    IPython.display.display(IPython.display.Audio(data=waveform, rate=sr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text generation\n",
    "We load the model for text generation to generate responses and the corresponding tokeniser to encode the input and decode the output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer: transformers.GPT2Tokenizer = AutoTokenizer.from_pretrained(lm_model_checkpoint_path)\n",
    "text_model: transformers.GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(tokeniser_checkpoint_path)\n",
    "text_model.to(device)\n",
    "text_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Speech synthesis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Spectrogram predictor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tts_configs: AttrDict = load_config(tts_model_configs_path)\n",
    "tts_configs.forward_attn_mask = True\n",
    "if 'characters' in tts_configs.keys():\n",
    "    symbols, phonemes = make_symbols(**tts_configs.characters)\n",
    "n_chars = len(phonemes) if tts_configs.use_phonemes else len(symbols)\n",
    "tts_configs.audio['stats_path'] = tts_model_scale_stats_path\n",
    "\n",
    "tts_ap = AudioProcessor(**tts_configs.audio)\n",
    "\n",
    "if tts_configs.use_external_speaker_embedding_file:\n",
    "    speaker_mapping = json.load(open(tts_model_speaker_file, 'r'))\n",
    "    n_speakers = len(speaker_mapping)\n",
    "    speaker_file_id = list(speaker_mapping.keys())[random.choice(range(n_speakers))]  # FIXME Select random speaker\n",
    "    speaker_embedding = speaker_mapping[speaker_file_id]['embedding']\n",
    "else:\n",
    "    n_speakers = 0  # FIXME this will cause a loading issue\n",
    "\n",
    "tts_model: Tacotron2 = setup_model(n_chars, n_speakers, tts_configs, speaker_embedding_dim=speaker_embedding_size)\n",
    "tts_model, _ = load_checkpoint(tts_model, tts_model_checkpoint_path, use_cuda=torch.cuda.is_available())\n",
    "# tts_model.load_state_dict(torch.load(tts_model_checkpoint_path, map_location=torch.device('cpu'))['model'])\n",
    "# tts_model.decoder.set_r(torch.load(tts_model_checkpoint_path, map_location=torch.device('cpu'))['r'])  #TODO see if correct/useful\n",
    "tts_model.to(device)\n",
    "tts_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Global Style Token\n",
    "We load the GST estimator to extract the style from a reference audio.\n",
    "Actually the GST estimator is already loaded as part of the Tacotron 2 spectrogram predictor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gst_model:  ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Speaker encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speaker_encoder_model.load_model(Path(speaker_encoder_model_checkpoint_path), device=device.type)\n",
    "# speaker_encoder_model.preprocess_wav\n",
    "# speaker_encoder_model.embed_utterance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Vocoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocoder_configs: AttrDict = load_config(vocoder_model_config_path)\n",
    "vocoder_configs.audio['stats_path'] = vocoder_model_scale_stats_path\n",
    "\n",
    "vocoder_ap = AudioProcessor(**vocoder_configs.audio)\n",
    "\n",
    "vocoder_model: FullbandMelganGenerator = setup_generator(vocoder_configs)\n",
    "vocoder_model.load_state_dict(torch.load(vocoder_model_checkpoint_path, map_location=torch.device('cpu'))['model'])\n",
    "vocoder_model.remove_weight_norm()\n",
    "vocoder_model.inference_padding = 0\n",
    "vocoder_model.to(device)\n",
    "vocoder_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GST prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gstt_model = ...\n",
    "gstt_model.to(device)\n",
    "gstt_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IEMOCAP\n",
    "We load the dialogues and audio file paths of IEMOCAP to provide some examples and randomly select one sample."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set = IEMOCAP()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IEMOCAP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get a random sample from IEMOCAP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context, response, audio_file_path = data_set[random.randint(0, len(data_set) - 1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Original"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show original dialogue and play original response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_dialogue(context, response)\n",
    "play_audio()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate audio using original GST and original response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_response_audio = tts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show original dialogue and play synthesised response with predicted GST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicted GST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encode dialogue context and response into contextual embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the GST from the contextual embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate audio using predicted GST and original response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show original dialogue and play synthesised response with predicted GST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_dialogue(context, response)\n",
    "play_audio()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicted response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a response using the LM (return also sequence of contextual embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate audio using original GST and predicted response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show dialogue with generated response and play synthesised response with original GST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_dialogue(context, )\n",
    "play_audio()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicted GST and response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a response using the LM (return also sequence of contextual embeddings).\n",
    "(Generation is done only if previous step was skipped)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the GST from the contextual embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate audio using predicted GST and predicted response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show dialogue with generated response and play synthesised response with predicted GST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_dialogue(context, )\n",
    "play_audio()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predicted VS. original GST visualisation\n",
    "Visualisation of the two GSTs on the same (original) response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(nrows=1, ncols=2, figsize=(w, h), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "ax = axes[0]\n",
    "...\n",
    "\n",
    "ax = axes[1]\n",
    "...\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Context\n",
    "Gather input dialogue context inserting one line at the time.\n",
    "Enter an empty line to stop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context = []\n",
    "while turn := input('>>> '):\n",
    "    context.append(turn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a response using the LM (return also sequence of contextual embeddings)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Audio synthesis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the GST from the contextual embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate audio using predicted GST and predicted response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "waveform = tts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show dialogue with generated response and play synthesised response with predicted GST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print_dialogue(context, )\n",
    "play_audio()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}